{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WVcdaipdmw2M","outputId":"e1110804-dd26-463c-ac7c-4889c9be6005","executionInfo":{"status":"ok","timestamp":1727154628621,"user_tz":-480,"elapsed":1184415,"user":{"displayName":"P. Zheng","userId":"08572117504573477537"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/rwalk/gsdmm.git\n","  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-byict91r\n","  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-byict91r\n","  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.26.4)\n","Building wheels for collected packages: gsdmm\n","  Building wheel for gsdmm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gsdmm: filename=gsdmm-0.1-py3-none-any.whl size=4586 sha256=98eac7861c510da2054c0050fb2f97d73c47ab9c290e63ab63a54cbd5db3676e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-aa30g526/wheels/da/d3/6e/a612d7cff0fcfb6470b8c113fc04931ecffb466ac19b9c5f3c\n","Successfully built gsdmm\n","Installing collected packages: gsdmm\n","Successfully installed gsdmm-0.1\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.1.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n","Collecting funcy (from pyLDAvis)\n","  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.4)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-2.0 pyLDAvis-3.4.1\n","Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-26-24] vedal987.csv\n","\n","Topic 0: Sample Messages for Review\n","sniffa sniffa foot\n","thought\n","cod zombie\n","fillygillyl fillyclap\n","abomination\n","\n","Topic 1: Sample Messages for Review\n","cooked\n","abomination\n","backslash\n","lot\n","eliv\n","\n","Topic 2: Sample Messages for Review\n","preference style animation\n","collab\n","wording\n","everyone cheese\n","fufu force fufublast fufu force fufublast fufu force fufublast fufu force fufublast fufu force fufublast fufu force fufublast fufu force fufublast\n","\n","Topic 3: Sample Messages for Review\n","reallygunpull mortis\n","character persona royal\n","person\n","trick frick\n","sound\n","\n","Topic 4: Sample Messages for Review\n","rule\n","reast\n","product sponsor\n","corpa\n","batchest\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ae9b699a-ffe2-4580-b13b-3510fbeac6d2\", \"cleaned_anonymized_[3-26-24] vedal987.csv_report.pdf\", 651266)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[1-5-24] vedal987.csv\n","\n","Topic 0: Sample Messages for Review\n","morning\n","time loop nuero\n","pepew life\n","let sadge\n","catjam tick tock\n","\n","Topic 1: Sample Messages for Review\n","reallygunpull\n","vtuber vtuber vtuber vtuber vtuber vtuber\n","vtuber vtuber vtuber vtuber vtuber vtuber vtuber vtuber\n","angelthump\n","vedalbedge\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1996d36f-381f-46f1-88c2-d43d7f9c4f14\", \"cleaned_anonymized_[1-5-24] vedal987.csv_report.pdf\", 164812)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[2-3-24] AI_RacingTV.csv\n","\n","Topic 0: Sample Messages for Review\n","waynetrain94 emka c841 p25\n","markovsky\n","affiliate request bit\n","st4n_bzh maserati\n","owner custom career mode application corsa\n","\n","Topic 1: Sample Messages for Review\n","race car wait car selection screen type race car whichever\n","waynetrain94 audi rs3 tcr\n","gary__z porsche hybrid\n","garage type car random gt300 jordan\n","mercedesbenz amg\n","\n","Topic 2: Sample Messages for Review\n","areyou_cereal\n","johnb073 community total\n","race progress race\n","areyou_cereal hill gh1\n","callaway corvette gt3r\n","\n","Topic 3: Sample Messages for Review\n","cydrec williams p10\n","deplorableapple race progress race\n","endikaboom bugatti eb110\n","waynetrain94 audi rs3 tcr\n","waynetrain94 asllani_9 hugrd03 jinxshootfaster cydrec everlastingapex v___ice st4n_bzh areyou_cereal anabiosis mro96\n","\n","Topic 4: Sample Messages for Review\n","stats stats win tournament crown event point betcoin win\n","discord date event team httpsdiscordggmcadjfk68z\n","gary__z\n","gary__z giulia\n","pot reward win win\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5c047c60-59ae-4b23-9996-5f2c167d70c5\", \"cleaned_anonymized_[2-3-24] AI_RacingTV.csv_report.pdf\", 665073)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-31-24] AI_RacingTV.csv\n","\n","Topic 10: Sample Messages for Review\n","\n","Topic 11: Sample Messages for Review\n","\n","Topic 6: Sample Messages for Review\n","\n","Topic 14: Sample Messages for Review\n","\n","Topic 16: Sample Messages for Review\n","Error in pyLDAvis visualization: Object of type complex is not JSON serializable\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_86b57456-23ab-4061-ab01-58c9bb0e2ebf\", \"cleaned_anonymized_[3-31-24] AI_RacingTV.csv_report.pdf\", 754817)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-21-24] TrumpOrBiden2024.csv\n","\n","Topic 0: Sample Messages for Review\n","hander guac throater\n","pooo\n","drink\n","crayon top twat waffle\n","scream\n","\n","Topic 1: Sample Messages for Review\n","teeth necklacecs phrase\n","supervinlin\n","trumpo1baby glitchcat\n","car\n","stroke\n","\n","Topic 2: Sample Messages for Review\n","fuel debate donate httpsstreamlabscomai_trumptip httpsstreamlabscomai_bidentip\n","hello candidate\n","fuel debate donate httpsstreamlabscomai_trumptip httpsstreamlabscomai_bidentip\n","fuel debate donate httpsstreamlabscomai_trumptip httpsstreamlabscomai_bidentip\n","fuel debate donate httpsstreamlabscomai_trumptip httpsstreamlabscomai_bidentip\n","\n","Topic 3: Sample Messages for Review\n","copping laptop subject longer\n","fair people tremendousness\n","thought people\n","gas0rass greta fix anything\n","backstreet boy frontstreet girl\n","\n","Topic 4: Sample Messages for Review\n","httpswwwredditcomraistreamfail\n","csgo cs2\n","activate\n","idiot\n","freejapanesecatgirls\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_11815229-755c-4eba-9f44-4014d90531a9\", \"cleaned_anonymized_[3-21-24] TrumpOrBiden2024.csv_report.pdf\", 443015)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-31-24] TrumpOrBiden2024.csv\n","\n","Topic 0: Sample Messages for Review\n","subsidy doodoo farm\n","resurrect harambe\n","ghost danger answer sound oooiiuuyyaawwewawweeiiuyyyuyyioooughhghghouiuioiughaweygheawaweoiuioiuuuioiouaghaghyghgayghaooouououououooouuuououuouououuigughughyyyyyooooooohhhhwiuoughyeghuioiuhyghewawewaey\n","nobody talk lizzos mommy milker\n","politics alter politician politics vote dude coolest name\n","\n","Topic 1: Sample Messages for Review\n","england\n","series biscuit\n","category pornhub\n","fragglemark ther alien\n","jobbaloon\n","\n","Topic 2: Sample Messages for Review\n","faceoff\n","urethra burn\n","line\n","drewpustkuchen something news incredibile\n","wtf learn haha\n","\n","Topic 3: Sample Messages for Review\n","impersonation roomba vomit lie son beau\n","reign destruction\n","witch pelosi\n","night kiss forehead\n","agent\n","\n","Topic 4: Sample Messages for Review\n","buzz\n","bigly\n","ellieg5swagstormy fallwinning\n","melon anime\n","nickname\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d66941de-ed7b-4c71-a53e-fc35f94ac7c7\", \"cleaned_anonymized_[3-31-24] TrumpOrBiden2024.csv_report.pdf\", 523003)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[4-1-24] AtheneAIHeroes.csv\n","\n","Topic 2: Sample Messages for Review\n","pucker face bidch salty face phuck eeeeeeh ooooaah waaaaaaaaaah ooaaeeeeeeeeehhh\n","jake tapper story avenger war vowel phrase\n","show game show news story schitt happens contestant insanity man weirdness place\n","plan tax spice spice tax vacation galaxy catch time someone try spice impact assessment triplicate\n","bapple tipple tipple dip dip dip ice cream chocolate hopple hipple zrt zrt zert wert tert fert fert\n","\n","Topic 7: Sample Messages for Review\n","fajitas fuh bar prancing pony earth\n","wait name bear\n","chicken nuggies gimme chicken nuggies gordon nuggies phat schitt floor nuggies nuggies b1r\n","friend crimsoncomet292 clone operation doppelgänger brightness mission crimson kockkockkock kockkock sentence ohphuckoh schitt\n","hair match color hair organ leg man ooooooooooouuuuuuuuyyyaaa\n","\n","Topic 4: Sample Messages for Review\n","legend chupacabras tale bedroom noun word response\n","desperate gaming computer dad acquaintance event minecraft entity game reality chaos fire computer head something time\n","obama technicien time yes time equipment anything president fuhck man sorry man\n","joke bird time joke\n","time wall paradis pay\n","\n","Topic 8: Sample Messages for Review\n","disrepect cam cob time blockbuster champ message champion non dumper oooooh oooeeoeoeoeeeeeoooeeeeeooeeeeeoo oooh holy phucc kock chocolate kock\n","time bathroom look turd toilet poop bidet bidet poop smell cabbage pringles bidet ahhhh\n","story bro tale copy drive jump drive drove bank jump drive deposit box future kid drive reader story\n","end name\n","list donate summon guest snoop dogg donate httpsstreamlabscomatheneaiheroestip donation message name list httpspastebincomgfbrbz8a work\n","\n","Topic 0: Sample Messages for Review\n","trudeaus top moment\n","usa minute ooo yes booboobubooboobooboobooboboobbooobooboobooboobooboobooboobooboobooboobuboob pause\n","hayes sheriff jury duty\n","rooster bayley iyo sky bucket spicy wing woman title food chain government\n","everything yeerks\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_791c40a4-27c8-47c5-bc86-cf59c49e89ac\", \"cleaned_anonymized_[4-1-24] AtheneAIHeroes.csv_report.pdf\", 642363)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-21-24] AtheneAIHeroes.csv\n","\n","Topic 7: Sample Messages for Review\n","gay bomb booger\n","globalist diatribe\n","water supply phuck supplement water supply vision globalist vampire\n","ice cream mmm bite haha laxative yer gon town brown take gamer space response\n","voice ahhahaha\n","\n","Topic 6: Sample Messages for Review\n","document world secret god bidch huh huuh hooh phuck phuck\n","cherry ballz\n","pacer test multistage capacity test pacer test line start speed beep lap time sound line ben\n","service announcement someone jest joke level style humor indication lol colon face\n","news document teleprompter hole creamy dragon dildoe phuck pee ness whack face\n","\n","Topic 1: Sample Messages for Review\n","discord\n","discord\n","discord\n","bro stuff toodoo potato lord lady potato mayo hybrid evandoood man stuff potato lord lady potato mayo hybrid jamie wow man evandood\n","snoop foggy dog chance smoking man\n","\n","Topic 9: Sample Messages for Review\n","town girl livin world train city boy train singer room smell wine perfumefor smile share nightit coffee\n","knut knut slap tapp knut knut tapp tapp knut knut slapper tapper slapper tapper knut slap knut slapper slapper anymore knut\n","elon musk reaction message civilization string eeeeeeeeeeeeee oooooooooo\n","recipe tomato soup chicken tikka masala\n","guy heap stuff\n","\n","Topic 3: Sample Messages for Review\n","tell story head hair head teeth twin brother balloon knot toilet\n","nuggies macawoney wight phuckin wait gimme nuggies bed waaaaaaaaaaaaaaaa weehhhhhh aaaah ooooh waaaaaaaaaaaaa oooooh\n","flem phuck bro body litre half acid sizzlin tss water intestine turnin virus nugget bro bid\n","time argument moon cheese food restaurant cheeseburger\n","hold hold echo bakbakbakbakbakohohohoohoohoohoohooohoooohhoooohhtuuuucccckkkeeerrrrrrsssooon\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_edf3caf0-3335-428c-9169-ceb9c65b3732\", \"cleaned_anonymized_[3-21-24] AtheneAIHeroes.csv_report.pdf\", 656056)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[2-29-24] AiTelevision.csv\n","\n","Topic 0: Sample Messages for Review\n","business venture club gazastrip\n","request analysis\n","type img description story story description svideo story httpsdiscordggb3jsxhjnet reset youtube httpswwwyoutubecomplaylistlistpldnq2lbrvoaltduyrtpidhicqleaf5cfn\n","svideo boy doctor blood doctor yell doctor\n","img standing field flower flower leica summicron f20 portra film grain\n","\n","Topic 1: Sample Messages for Review\n","story character party guest std character history guest participant list condition history detail state name condition side effect condition detail condition forever\n","story speculum morty catch morty see speculum beth answer alien describes alien detail size texture type std\n","story adult episode squarepants bass town krab bubblebass order squidward order demand order bass point ingredient spongebob circuit mind episode\n","reason food work\n","svideo party game reallife celebrity character mascot list dimension detail state dimension letter point dimension vivid detail\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2317bfb4-10db-47a9-ba98-e84109b9dc20\", \"cleaned_anonymized_[2-29-24] AiTelevision.csv_report.pdf\", 277972)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[2-9-24] AiTelevision.csv\n","\n","Topic 0: Sample Messages for Review\n","story morty snort alcohol class speech dialogue alcohol speech morty dialogue comment student student name morty woman tinder visit\n","story morty game waifus\n","story dive chowder show chowder friend appearance rhyming rap battle detail chowder friend pit version shrek stanky waxy ear friend punishment detail end\n","vlioentlymurders gun\n","story movie horror scene man bathroom trap cow body detail message jigsaw trap epic story man jigsaw character story man\n","\n","Topic 1: Sample Messages for Review\n","goood lord\n","story speculum morty catch morty see speculum beth answer\n","mine\n","jasontheartist request analysis harassment\n","request analysis\n","\n","Topic 2: Sample Messages for Review\n","character zaire emery lamine character everyone bfdi zaire emery leny yoro\n","openai\n","story morty alphabet game letter oppai waifu manko interrupt knife moment\n","rest boiler plate\n","svideo party game reallife celebrity character mascot list dimension detail state dimension letter point dimension vivid detail\n","\n","Topic 3: Sample Messages for Review\n","record\n","demon\n","story work barista drink starbucks customer order drink relationship status price item drink complains customer customer crisis medication\n","story horror roach morty try powerless rick colon roach\n","morty delorean baby baby lever morty pretend thanks\n","\n","Topic 4: Sample Messages for Review\n","prmpt\n","outta\n","img episode character letter letter line narrator word letter\n","warcrimes\n","boost viewer dogehype com\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_44871e40-ee4d-4fdf-8985-d91839ac2bd7\", \"cleaned_anonymized_[2-9-24] AiTelevision.csv_report.pdf\", 651598)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-31-24] ask_jesus.csv\n","\n","Topic 0: Sample Messages for Review\n","love someone\n","jehoshaphat cookware\n","type sense humor god vegan building ground guess sisity funny\n","mary\n","brother gabriel gay\n","\n","Topic 1: Sample Messages for Review\n","fry egg soccer lingo\n","counter strike team premiere match\n","time uncle death father father sky balance wait lion\n","god toilet\n","woman\n","\n","Topic 2: Sample Messages for Review\n","exclamation mark\n","muhammad spam hahaha\n","project chip server infrastructure cost\n","project chip server infrastructure cost\n","project chip server infrastructure cost\n","\n","Topic 3: Sample Messages for Review\n","start message\n","thought malevelon creek victory threat\n","genesis lot daughter person city section view\n","voice playht text generator voice section link httpsplayht\n","alien pirate eyesight way men amen\n","\n","Topic 4: Sample Messages for Review\n","screen\n","vaccine\n","pokemon request everyone wisdom paramount understanding teaching\n","swissqwertz\n","teaching short viewer\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_86f14f32-64f6-4f66-839e-8964a8f452a0\", \"cleaned_anonymized_[3-31-24] ask_jesus.csv_report.pdf\", 639956)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/My Drive/Twitch dataset/Cleaned data/cleaned_anonymized_[3-21-24] ask_jesus.csv\n","\n","Topic 0: Sample Messages for Review\n","brother people death experience body experience\n","fear people\n","people\n","didint people death people people word criminal\n","alex life\n","\n","Topic 1: Sample Messages for Review\n","application form httpsformsgle3qujcwhywnbthn866\n","religion application form application membership card\n","facebook advice person\n","cheat code motherlode sims\n","tip exam tip\n","\n","Topic 2: Sample Messages for Review\n","demon\n","enough bottle\n","story guy world record space alphabet order response thanks way response\n","ban ticket ramen\n","breathes\n","\n","Topic 3: Sample Messages for Review\n","nonbeliever\n","restaurant dish salmon rice seaweed alarming fish complain amen\n","mission philip story character trevor phillips opinion cutscene mission ramen buddha\n","voice playht text generator voice section link httpsplayht\n","glitchcat\n","\n","Topic 4: Sample Messages for Review\n","satan\n","gon degree\n","man devil god monkey\n","shatan people demtia reply space bracket bear gymmy brain gymmy brain worm inclusive\n","asianbunnyx idea pray man\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_6cd9d314-b89c-4596-b096-64ac801e9255\", \"cleaned_anonymized_[3-21-24] ask_jesus.csv_report.pdf\", 567188)"]},"metadata":{}}],"source":["!pip install git+https://github.com/rwalk/gsdmm.git\n","!pip install nltk spacy gensim pyLDAvis networkx wordcloud\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import random\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import pos_tag\n","import nltk\n","import spacy\n","import gensim\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","import networkx as nx\n","from collections import Counter\n","from itertools import combinations\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Load Spacy's English model for Named Entity Recognition\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Mount Google Drive to access the dataset stored there\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Download necessary NLTK resources for tokenization and POS tagging\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Enhance the stop words list with terms commonly irrelevant in casual discussions\n","additional_stopwords = {'get', 'okay', 'oh', 'yeah', 'hey', 'hi', 'please', 'thank', 'welcome', 'like', 'just', 'know', 'really', 'thing', 'things', 'said', 'also', 'one', 'use', 'using', 'used'}\n","\n","# Context-specific terms related to AI-generated content discussions\n","contextual_stopwords = {'help', 'think', 'probably', 'topic', 'question', 'ask', 'need'}\n","\n","# AI-generated content specific terms\n","aigc_stopwords = {'ai', 'artificial', 'intelligence', 'generated', 'generate', 'generation', 'content', 'image', 'images', 'video', 'videos', 'picture', 'pictures', 'photo', 'photos', 'graphic', 'graphics', 'stable', 'diffusion', 'model', 'models'}\n","\n","# Platform-specific terms related to Twitch discussions\n","twitch_stopwords = {'vedal987', 'racingtv', 'ask', 'jesus', 'trump', 'biden', '2024', 'athene', 'aiheroes', 'aitelevision', 'stream', 'streaming', 'live', 'channel', 'twitch', 'sub', 'subscribe', 'follow', 'followers', 'chat', 'chats', 'bot', 'bots', 'mod', 'mods', 'moderator', 'moderators', 'emote', 'emotes', 'badge', 'badges', 'hype', 'raid', 'raids', 'clip', 'clips', 'vod', 'vods'}\n","\n","# Combine all sets of stop words\n","stop_words = set(stopwords.words('english')) | additional_stopwords | aigc_stopwords | contextual_stopwords | twitch_stopwords\n","\n","# Define the preprocessing function\n","def preprocess(text):\n","    lemmatizer = WordNetLemmatizer()\n","    # Remove all non-word characters and lower the text\n","    text = re.sub(r'[^\\w\\s]', '', text.lower())\n","    # Split text into tokens\n","    tokens = text.split()\n","    # Apply NER to remove usernames and named entities\n","    doc = nlp(' '.join(tokens))\n","    tokens = [token.text for token in doc if not token.ent_type_]\n","    # POS 的位置提前了，本来在最后\n","    # Retain only nouns as they are significant for topic identification\n","    nouns = [word for word, pos in pos_tag(tokens) if pos.startswith('NN')]\n","    # 筛选本来 lemmatize 和 len(word) > 2 是在同一行的，有可能 lemmatize 之后 word length 小于 2，所以这里把长度筛选放在后面了\n","    # Lemmatize tokens and remove stop words and short words\n","    nouns = [lemmatizer.lemmatize(word) for word in nouns]\n","    nouns = [word for word in nouns if word not in stop_words and len(word) > 2]\n","    return nouns\n","\n","# Function to load and preprocess data from CSV files\n","def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    df['preprocessed'] = df['Message'].apply(preprocess)\n","    return df['preprocessed'].tolist()\n","\n","# Path to the folder containing the dataset\n","folder_path = '/content/drive/My Drive/Twitch dataset/Cleaned data/'\n","file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n","\n","# Function to evaluate coherence\n","def evaluate_coherence(dictionary, docs, limit, start=2, step=3):\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in range(start, limit, step):\n","        model = gensim.models.ldamodel.LdaModel(corpus=[dictionary.doc2bow(doc) for doc in docs],\n","                                                id2word=dictionary,\n","                                                num_topics=num_topics,\n","                                                random_state=100,\n","                                                update_every=1,\n","                                                chunksize=100,\n","                                                passes=10,\n","                                                alpha='auto',\n","                                                per_word_topics=True)\n","        model_list.append(model)\n","        coherencemodel = gensim.models.CoherenceModel(model=model, texts=docs, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","    return model_list, coherence_values\n","\n","# Function to randomly select messages from each topic for manual review\n","def random_message_selection(docs, best_model, dictionary, num_samples=5):\n","    topics = best_model.show_topics(num_words=10, formatted=False)\n","    for topic_id, _ in topics[:5]:  # Limit to top 5 topics\n","        topic_docs = [doc for doc in docs if any(topic_id == topic[0] for topic in best_model.get_document_topics(dictionary.doc2bow(doc), minimum_probability=0.1))]\n","        sampled_messages = random.sample(topic_docs, min(len(topic_docs), num_samples))\n","        print(f\"\\nTopic {topic_id}: Sample Messages for Review\")\n","        for message in sampled_messages:\n","            print(' '.join(message))\n","\n","# Function to save plots to a PDF\n","def save_plots_to_pdf(file_path, best_model, dictionary, top_clusters):\n","    pdf_file_name = f'{os.path.basename(file_path)}_report.pdf'\n","    with PdfPages(pdf_file_name) as pdf:\n","        for topic_id in top_clusters:\n","            # Word cloud\n","            topic_words = dict(best_model.show_topic(topic_id, topn=50))\n","            wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(topic_words)\n","            plt.figure(figsize=(10, 5))\n","            plt.imshow(wordcloud, interpolation='bilinear')\n","            plt.title(f\"Topic {topic_id} Word Cloud for {os.path.basename(file_path)}\")\n","            plt.axis(\"off\")\n","            pdf.savefig()\n","            plt.close()\n","\n","            # Bar chart\n","            sorted_topic_words = dict(sorted(topic_words.items(), key=lambda item: item[1], reverse=True)[:10])\n","            plt.figure(figsize=(10, 5))\n","            plt.bar(sorted_topic_words.keys(), sorted_topic_words.values())\n","            plt.title(f\"Top Words in Topic {topic_id}\")\n","            plt.xticks(rotation=45)\n","            pdf.savefig()\n","            plt.close()\n","\n","            # Network graph\n","            words = list(sorted_topic_words.keys())\n","            word_pairs = list(combinations(words, 2))\n","            G = nx.Graph()\n","            G.add_edges_from(word_pairs)\n","            pos = nx.spring_layout(G)\n","            plt.figure(figsize=(10, 5))\n","            nx.draw(G, pos, with_labels=True, node_size=50, font_size=10, edge_color='grey')\n","            plt.title(f\"Word Co-occurrence Network for Topic {topic_id}\")\n","            pdf.savefig()\n","            plt.close()\n","\n","    return pdf_file_name\n","\n","# Process each file, perform topic modeling, and generate visualizations\n","for file_path in file_paths:\n","    print(f\"Processing file: {file_path}\")\n","    docs = load_data(file_path)\n","    # Remove any empty documents resulting from preprocessing\n","    docs = [doc for doc in docs if doc]\n","\n","    if len(docs) > 0:\n","        # Create a dictionary of all words in documents\n","        dictionary = gensim.corpora.Dictionary(docs)\n","        # Remove extremes to focus on relevant words only\n","        dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=10000)\n","        vocab_length = len(dictionary)\n","\n","        if vocab_length == 0:\n","            print(\"No vocabulary available after filtering extremes. Skipping.\")\n","            continue\n","\n","        # Evaluate coherence for different number of topics\n","        model_list, coherence_values = evaluate_coherence(dictionary, docs, limit=20)  # Increase to explore more topics\n","\n","        # Select the model with the highest coherence\n","        best_model_index = coherence_values.index(max(coherence_values))\n","        best_model = model_list[best_model_index]\n","\n","        # Random message selection for validation\n","        random_message_selection(docs, best_model, dictionary)\n","\n","        # Visualize the topics using pyLDAvis\n","        try:\n","            lda_display = gensimvis.prepare(best_model, [dictionary.doc2bow(doc) for doc in docs], dictionary)\n","            pyLDAvis.display(lda_display)\n","        except Exception as e:\n","            print(f\"Error in pyLDAvis visualization: {e}\")\n","\n","        # Generate and display word clouds for each of the top 5 topics\n","        sorted_clusters = sorted(range(best_model.num_topics), key=lambda k: sum([tup[1] for tup in best_model.get_topic_terms(k)]), reverse=True)\n","        top_clusters = sorted_clusters[:5]\n","\n","        # Save plots to PDF\n","        pdf_file_name = save_plots_to_pdf(file_path, best_model, dictionary, top_clusters)\n","\n","        # Provide a download link for the PDF file\n","        from google.colab import files\n","        files.download(pdf_file_name)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"15UZTlV4gsVsWUXQya2WX-mze_4hReEJq","timestamp":1719199539796},{"file_id":"1UdquvvoHdUVzngZ28AiEhiGWx8_2o3ot","timestamp":1719199409797},{"file_id":"1VLhZV2sFW74CYtwn69AQMbr3_YLPo0Y0","timestamp":1718895132889},{"file_id":"1Bh0HDMF8ZQ_g7kLYeDNkj-fUKM_xdNLT","timestamp":1718892553507},{"file_id":"1RsqDA8cwPvR3OL-0y3SC5TqOjFo-9jWZ","timestamp":1718065641092},{"file_id":"19OqztM7fQk-v_gqRMYsg-Ukc7lDMRZDn","timestamp":1717756029481},{"file_id":"1ek0mOFIw9q-kjfEKMajqtJW4aSvc1fCO","timestamp":1716801897822},{"file_id":"1f76zgQRHexaJ_-PpCKy738EGGovwXXl4","timestamp":1716242565360},{"file_id":"1JityMIM3UOpBL_DOPOFT3wOALMahhu6w","timestamp":1716238257258},{"file_id":"1YsBuErRqAjoWhDGj89Bt_KIjxeaDgVwM","timestamp":1714138761396},{"file_id":"1fiFhZtpqaOFhxxPJ0mkuu4xC9e_PCjAD","timestamp":1713823781944},{"file_id":"1YHTYFFzjyHHDUa28QvbKg0u1ecE0YFvU","timestamp":1713822045554},{"file_id":"1hphNrR393EQIib44_VspqKJy2MLPQhlZ","timestamp":1713821029465},{"file_id":"1TGcNhpyl6h5Tqhl6Dxis-dGyxOKllPjR","timestamp":1713274950388},{"file_id":"1tEZ5i5C9LAXenvpWOzApobMrn9u01hAT","timestamp":1712788945026},{"file_id":"1GYvnuvBGpY_VKbxPMvx4o_ufreub5jSh","timestamp":1712788076006},{"file_id":"13c7jdPO1fMBoSvL3g0efqSpAlb3rQawx","timestamp":1712786634517}],"authorship_tag":"ABX9TyNjQmE9bO5vp3GcOynuFADz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}